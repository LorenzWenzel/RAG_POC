# POC eines RAG System â€“ DSGVO-konforme Vertragsanalyse

Dieses Projekt ist ein **Retrieval-Augmented Generation (RAG) System**, das auf **Streamlit** basiert und vollstÃ¤ndig **DSGVO-konform** arbeitet.  
Es ermÃ¶glicht sowohl die **Analyse** als auch den **interaktiven Chat** mit hochgeladenen Dokumenten â€“ lokal oder containerisiert via **Docker**.

---

## ğŸ“¸ Ãœbersicht

![Front Page](demo/first_page.png)

Die Startseite bietet zwei Hauptfunktionen:

- **ğŸ“¤ Upload Chat:** Hochladen und Verarbeiten von Dokumenten  
- **ğŸ’¬ RAG Chat:** Semantische Suche und Konversation Ã¼ber eingebettete Inhalte  

---

## ğŸ¥ Demo-Videos

**RAG Chat**  
[â–¶ï¸ Rag_Chat.mov](demo/Rag_Chat.mov)

**Upload Chat**  
[â–¶ï¸ UploadChat.mov](demo/UploadChat.mov)

---

## âš™ï¸ Architektur & Features

- **Frontend:** Streamlit (Multi-Page App mit moderner UI)
- **Backend:** Python mit modularer Struktur (`RAG_POC.py`, `SectionBuilder.py`, `UploadAnswer.py`)
- **Embeddings & Retrieval:** Azure Cognitive Search / OpenAI Embeddings  
- **Dokumentenverarbeitung:** Azure Document Intelligence (OCR & Strukturierung)
- **Datenschutz:** DSGVO-konform durch lokale oder private Cloud-Verarbeitung
- **Containerisierung:** LauffÃ¤hig in Docker oder via Docker Compose
- **Erweiterbarkeit:** Sauber getrennte Module, leicht erweiterbar um neue Pipelines oder Modelle

---

## ğŸ› ï¸ Technik (Wie das RAG funktioniert)

### 1) Import & OCR
- **Quelle:** Dateien aus `./Input` (`.pdf`, `.png`, `.jpg`, `.tif` â€¦).  
- **OCR:** Asynchroner Batch mit `azure.ai.documentintelligence.aio.DocumentIntelligenceClient` (`prebuilt-read`), gesteuert Ã¼ber `MAX_CONCURRENCY=10`.  

### 2) Chunking & Headings
- **Tokenizer-basierte Chunking-Engine** (`chunk_tokenwise_with_line_snap`):  
  - ZielgrÃ¶ÃŸe ~800 Tokens, Overlap ~100, weich auf **ZeilenanfÃ¤nge** und **Satzenden** â€œsnappenâ€.  
  - Erkennung von **Â§-Ãœberschriften** (Regex), die als zusÃ¤tzliche Headings pro Chunk mitgefÃ¼hrt werden.  
- Ergebnis: Liste `(heading, text)`-Paare je Dokument; persistiert in `./Result`.

### 3) Metadaten-Extraktion (Low-Context, Vertragstitel/-kopf)
- **LLM-gestÃ¼tzt (Azure OpenAI, GPT-4.1-nano):**  
  - Extrahiert robuste Felder wie `property_code`, `unit_codes`, `street`, `house_no`, `postal_code`, `city`, `tenant`, `start_date`, `fixed_term_end`.  
  - Nutzt **nur** die ersten 1â€“2 Sections (Kopf/PrÃ¤ambel) zur Minimierung von Halluzinationen; normalisiert Schreibweisen (z. B. `WE 03.12`).

### 4) Embeddings & Indexierung
- **Embeddings:** Azure OpenAI (`text-embedding-3-small`, 1536 Dimensionen).  
- **Azure AI Search Index (`chunks`):**  
  - Felder: `id`, `doc_id`, `chunk_no`, `heading`, `content`, `meta{...}`, `embedding`.  
  - **VectorSearch:** HNSW + Profil `vprof`.  
  - **Semantic Search:** Konfiguration `default` mit `heading` als `title_field`, `content` als `content_fields` und priorisierten Keywordfeldern aus `meta` (z. B. `meta/property_code`, `meta/city`, `meta/unit_codes` â€¦).  
  - IDs & Keys werden **sicher normalisiert** (Umlaute â†’ `ae/oe/ue`, `ÃŸ` â†’ `ss`, erlaubte Zeichen).

### 5) Retrieval (Hybrid + Reranker)
- **Query-Pipeline:**  
  1. Erzeuge Query-Embedding.  
  2. **Hybrid-Search** in Azure AI Search: `search_text=query` (BM25) **+** `vector_queries=[embedding]`.  
  3. **Semantic Reranker** (QueryType `SEMANTIC`, Config `default`) sortiert Ergebnisse.  
  4. **per_doc_cap** begrenzt Treffer pro `doc_id` (Standard: 3), um **StreusÃ¤tze** zu vermeiden.  
  5. (Optional) **Meta-Filter**: iterativer, feldbasierter Substring-Match Ã¼ber `meta` (z. B. StraÃŸe, PLZ, `unit_codes`).

### 6) Antwortgenerierung (Grounded QA)
- **Kontextformatierung:** Nummerierte AuszÃ¼ge `[rank]` inkl. Quelle (`doc_id`), Heading, optional Seitenangaben.  
- **LLM (Azure OpenAI, GPT-5-nano):**  
  - Systemprompt erzwingt **Quellenbelege** `[ #rank ]` und **pure Kontexttreue** (â€œWenn unklar, sag es explizitâ€).  
  - `reasoning_effort="low"` und `max_completion_tokens` kontrollierbar.  
- **Speicherung:** Der **User Prompt** wird in `./userPrompt/prompt.txt` abgelegt (Revisions-/Audit-Zwecke).

### 7) Steuerung & Betriebsmodi
- **K & Capping:** `K` (Top-K), `topK0` (Server-Recall), `per_doc_cap` (DiversitÃ¤t) sind expose-bar.

### 8) DSGVO & Sicherheit
- **Keine personenbezogenen Daten nach auÃŸen**: Alle Schritte kÃ¶nnen lokal/privat laufen.  
- **.env-basierte Konfiguration**: Keys/Endpoints via `dotenv` geladen; **keine Secrets im Code/Repo**.  
- **Sanitizing & Limits**: Payload-Capping, saubere Filenamen, robuste Fehlerbehandlung im Async-Pfad.

> **Genutzte Services**  
> - **Azure Document Intelligence** (OCR, `prebuilt-read`)  
> - **Azure OpenAI** (Embeddings & Chat/Completion fÃ¼r Meta-Extraktion und Antworten)  
> - **Azure AI Search** (Hybrid Retrieval, Vektorindex, Semantic Reranking)  
> - **Streamlit** (Frontend)

---

## ğŸš€ How To Run

### ğŸ”§ Voraussetzungen
- Docker & Docker Compose installiert  
- `.env` Datei mit den benÃ¶tigten API Keys und Einstellungen vorhanden, wie in .env_template

### ğŸ“‚ Vorbereitung
1. Lege deine zu analysierenden Dateien im Ordner `Input/` ab.  
2. Optional: ÃœberprÃ¼fe oder passe die Mounts in `docker-compose.yml` an.  

### â–¶ï¸ Start mit Docker Compose

```bash
docker-compose up --build
